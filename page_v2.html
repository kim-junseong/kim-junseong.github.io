<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta name="google-site-verification" content="xDNWUvx6Q5EWK5YYSyKvK8DZTmvXhKsGX203Ll-BFFE" >
    <meta http-equiv="content-type" content="text/html; charset=utf-8"/>
    <title>Kim Junseong</title>
    <link rel="stylesheet" type="text/css" id="defaultstyle" href="./style.css"/>
    <script type="text/javascript" src="js/tracker.js"></script>
    <script type="text/javascript" src="js/pageturner.js"></script>
  </head>
  <script src="script/functions.js"></script>
  <body>
    <div class="container">

      <div class="intro">

        <img src="media/profile_240713.png" width="224" alt="Kim Junseong"/>

        <h1>Kim Junseong</h1>

        <p>
          I am a Ph.D. student at <a href="https://ami.postech.ac.kr/">AMILab</a> in Electrical Engineering at <a href="https://postech.ac.kr/eng">POSTECH</a>, advised by <a href="https://ami.postech.ac.kr/members/tae-hyun-oh">Tae-Hyun Oh</a>.
          Previously, I was a research scientist intern at researcher at <a href="https://virtualhumans.mpi-inf.mpg.de/">Real Virtual Humans Group</a> at the <a href="https://uni-tuebingen.de/">Univ. of Tübingen</a>, working with <a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html">Gerard Pons-Moll</a>.
<!--          I am also an incoming research scientist intern at <a href="https://about.meta.com/realitylabs/"> Meta Reality Labs</a>, hosted by <a href="https://www.linkedin.com/in/yaser-sheikh-9847a64/">Yaser Sheikh</a>.-->
        </p>
        <p>
          My reseatch lies in 3D machine perception including, 3D Reconstruction, Neural rendering, and Computational Photography, but not limited to.
<!--            During my bachelor, I spent a semester at <a href="https://www.ntu.edu.sg/">Nanyang Technological University (NTU)</a> as an exchange student.-->
        </p>
        <p>
          <a class="a_more" href="mailto:junseong.kim@postech.ac.kr">junseong.kim@postech.ac.kr</a> |
          <a class="a_more" href="https://www.overleaf.com/read/hcphxhfygpks">CV</a> |
          <a class="a_more" href="https://scholar.google.co.kr/citations?user=CMqYPY8AAAAJ&hl=ko">Google Scholar</a> |
          <a class="a_more" href="https://github.com/gucka28">GitHub</a>
        </p>

      </div> <!-- intro -->

      <h2>News</h2>
        <ul>
        <br>
          <li><b>10/2024 <span style="color:#BB2222;">(New!)</span></b> I'll be joining <a href="https://about.meta.com/realitylabs/">Meta Reality Labs - Codec Avatars team</a> as a research scientist intern. </li>
          <li><b>02/2024 <span style="color:#BB2222;">(New!)</span></b> Our paper on <a href="https://arxiv.org/abs/2312.11360">text-driven PBR texture synthesis</a> is accepted to CVPR 2024.</li>
          <li><b>12/2023</b> Our paper on efficient 3D scene stylization is accepted to AAAI 2024. </li>
          <li><b>11/2023</b> I won the Grand Prize (1st place, Minister's award, $12,000) at ICT paper awards. </li>
          <li><b>10/2023</b> I am selected as an <a href="https://ami.postech.ac.kr/57da3912-6dc3-41a1-b355-f010231a1756">outstanding reviewer</a> for ICCV 2023. </li>
          <li><b>10/2023</b> I am joining <a href="https://virtualhumans.mpi-inf.mpg.de/">Real Virtual Humans Group</a> as a visiting Ph.D. student. </li>
  <!--                show more toggle button-->
          <a class="a_more" href="javascript:toggleblock(&#39;old_news&#39;)">---- show more ----</a>
          <div id="old_news" style="display: none;">
            <li><b>08/2023</b> Our paper on text-driven human avatar generation is accepted to ICCVW 2023. </li>
            <li><b>04/2023</b> Our paper on 3D face mesh recon. on videos is accepted to CVPRW 2023. </li>
            <li><b>02/2023</b> Invited to give a talk on text-driven 4D human avatars at <a href="http://innerverz.io/">INNERVERZ</a>. </li>
            <li><b>01/2023</b> Our <a href="https://link.springer.com/article/10.1007/s00371-023-02798-x">paper</a> on lightweight body mesh recon. is accepted to TVCJ 2023. </li>
            <li><b>11/2022</b> Our paper <a href="https://clip-actor.github.io">CLIP-Actor</a> is selected as the winner of <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2022-south-korea">Qualcomm Innovation Fellowship Korea</a>!</li>
            <li><b>07/2022</b> Our two papers, <a href="https://clip-actor.github.io">CLIP-Actor</a> and <a href="https://fastmetro.github.io">FastMETRO</a>, are accepted to ECCV 2022.</li>
            <li><b>04/2022</b> I got accepted to <a href="https://iplab.dmi.unict.it/icvss2022/">ICVSS 2022</a>, and I will visit Sicily, Italy this summer. </li>
            <li><b>10/2021</b> Our paper <a href="https://demr.github.io">DeMR</a> is accepted to BMVC 2021.</li>
          </div>
        </ul>

        <h2>Research Experiences</h2>
            <ul>
                <li class="list-item">
                    <b>Meta Reality Labs - Codec Avatars team</b>, Pittsburgh, PA.<span style="float: right;">Oct 2024 - Mar 2025</span><br>
                    (Incoming) Research Scientist Intern, will work with <a href="https://www.linkedin.com/in/yaser-sheikh-9847a64/">Yaser Sheikh</a>, and <a href="https://www.linkedin.com/in/jovan-cmu/">Jovan Popović</a>.<br/>
                </li>
                Research Intern in 3D NeRF team @ NAVER Cloud (2023.02 ~ 2023.08)
                <li class="list-item">
                    <b>University of Tübingen - Real Virtual Humans group</b>, Tübingen, Germany.<span style="float: right;">Sep 2023 - Mar 2024</span><br>
                    Visiting Researcher, worked with <a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html">Gerard Pons-Moll</a>.<br/>
                </li>
                Research Collaboration with NVIDIA Taiwan (2024.02)
                <li class="list-item">
                    <b>POSTECH - Algorithmic Machine Intelligence lab</b>, Pohang, Korea.<span style="float: right;">Sep 2020 - Present</span><br>
                    Graduate Student, working with <a href="https://ami.postech.ac.kr/members/tae-hyun-oh">Tae-Hyun Oh</a>.<br/>
                </li>
            </ul>

      <h2>Publications</h2>

      <div class="item">
        <img src='./media/paint_it.png' width="192" />
        <p>
          <b>&#127912 Paint-it: Text-to-Texture Synthesis via Deep Convolutional Texture Map Optimization and Physically-Based Rendering</b><br/>
          <u>Kim Youwang</u>, Tae-Hyun Oh, Gerard Pons-Moll<br/>
          <i>CVPR 2024</i><br/>
            <a href="https://kim-youwang.github.io/paint-it">Project page</a> |
            <a href="https://arxiv.org/abs/2312.11360">Paper</a> |
            <a href="https://github.com/postech-ami/paint-it">Code</a>
        </p>
      </div>

      <div class="item">
        <img src='./media/clip-actor-x.jpg' width="192" />
        <p>
          <b>CLIP-Actor-X: Text-driven 4D Human Avatar Generation via Cross-modal Synthesis-through-Optimization</b><br/>
          <u>Kim Youwang</u>*, Taehyun Byun*, Kim Ji-Yeon, Sungjoon Choi, Tae-Hyun Oh<br/>
          <i>Journal under review</i><br/>
        </p>
      </div>

      <div class="item">
        <img src='./media/syn3d.png' width="192" />
        <p>
          <b>ObjectDR: Object-Centric Domain Randomization for 3D Shape Reconstruction in the Wild</b><br/>
          Junhyeong Cho, <u>Kim Youwang</u>, Hunmin Yang, Tae-Hyun Oh<br/>
          <i>ArXiv, 2024</i><br/>
            <a href="https://objectdr.github.io/">Project page</a> |
            <a href="https://arxiv.org/abs/2403.14539">Paper</a>
        </p>
      </div>


      <div class="item">
        <img src='./media/obj_mesh.gif' width="192" />
        <p>
          <b>(Ongoing) Monocular 3D object mesh reconstruction</b><br/>
          <br/>
          <i>Work in progress</i><br/>
        </p>
      </div>


      <div class="item">
        <img src='./media/fprf.gif' width="192" />
        <p>
          <b>Feed-Forward Photorealistic Style Transfer for Large-Scale 3D Neural Radiance Fields</b><br/>
          GeonU Kim, <u>Kim Youwang</u>, Tae-Hyun Oh<br/>
          <i>AAAI 2024</i><br/>
            <a href="https://kim-geonu.github.io/FPRF/">Project page</a> |
            <a href="https://arxiv.org/abs/2401.05516">Paper</a> |
            <a href="https://github.com/postech-ami/FPRF">Code</a>
        </p>
      </div>

      <div class="item">
        <img src='./media/4d_face.gif' width="192" />
        <p>
          <b>A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized Optimization</b><br/>
          <u>Kim Youwang</u>, Lee Hyun*, Kim Sung-Bin*, Suekyeong Nam, Janghoon Ju, Tae-Hyun Oh<br/>
          <i>ArXiv, 2023</i><br/>
            <a href="https://neuface-dataset.github.io">Project page</a> |
            <a href="https://arxiv.org/abs/2310.03205">Paper</a>
        </p>
      </div>

      <div class="item">
        <img src='./media/tex_avatar.gif' width="192" />
        <p>
          <b>Text-driven Human Avatar Generation by Neural Re-parameterized Texture Optimization</b><br/>
          <u>Kim Youwang</u>, Tae-Hyun Oh<br/>
          <i>ICCVW 2023</i><br/>
        </p>
      </div>

      <div class="item">
        <img src='./media/stream.gif' width="192" />
        <p>
          <b>STREAM: Spatio-Temporally Consistent Face Mesh Reconstruction on Videos</b><br/>
          <u>Kim Youwang</u>, Lee Hyun*, Kim Sung-Bin*, Suekyeong Nam, Janghoon Ju, Tae-Hyun Oh<br/>
          <i>CVPRW 2023</i><br/>
        </p>
      </div>

      <div class="item">
        <img src='./media/rankpruning.jpg' width="192" />
        <p>
          <b>Multi-stage Adaptive Rank Statistic Pruning for Lightweight Human 3D Mesh Recovery Model</b><br/>
          Dong Hun Ryou, <u>Kim Youwang</u>, Tae-Hyun Oh<br/>
          <i>The Visual Computer Journal (TVCJ) 2023</i><br/>
            <a href="https://link.springer.com/article/10.1007/s00371-023-02798-x">Paper</a>
        </p>
      </div>

      <div class="item">
        <img src='./media/clip_actor.gif' width="192" />
        <p>
          <b>CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes</b><br/>
          <u>Kim Youwang</u>*, Kim Ji-Yeon*, Tae-Hyun Oh<br/>
          <i>ECCV 2022</i><br/>
            <a href="https://clip-actor.github.io">Project page</a> |
            <a href="https://arxiv.org/abs/2206.04382">Paper</a> |
            <a href="https://youtu.be/oWr4NP-eVLY">Video</a> |
            <a href="https://github.com/postech-ami/CLIP-Actor">Code</a> |
            <a href="https://www.dropbox.com/s/8l2jvvc0po6szn7/3229-poster.pdf?dl=0">Poster</a><br/>
            <span style="color:#BB2222;">- Grand Prize (1st place, Minister's award, $12,000) at ICT paper awards 2023 </span><br/>
            <span style="color:#BB2222;">- Qualcomm Innovation Award Winner 2022</span>
        </p>
      </div>

      <div class="item">
        <img src='./media/fastmetro_attention.png' width="192" />
        <p>
          <b>FastMETRO: Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers</b><br/>
          Junhyeong Cho, <u>Kim Youwang</u>, Tae-Hyun Oh<br/>
          <i>ECCV 2022</i><br/>
            <a href="https://fastmetro.github.io/">Project page</a> |
            <a href="https://arxiv.org/abs/2207.13820">Paper</a> |
            <a href="https://github.com/postech-ami/FastMETRO">Code</a> |
            <a href="https://www.dropbox.com/s/kzmihz488qcelxi/2116-poster.pdf?dl=0">Poster</a><br/>
        </p>
      </div>

      <div class="item">
        <img src='./media/icml24.png' width="192" />
        <p>
        <b>Synergistic Integration of Coordinate Network and Tensorial Feature for Improving Neural Radiance Fields from Sparse Inputs</b><br/>
        <u>Mingyu Kim, Kim Jun-Seong</u>, Se-Young Yun, Jin-Hwa Kim <br/>
            <i>ICML 2024</i><br/>
            <a href="https://mingyukim87.github.io/SynergyNeRF/">Project page</a> |
            <a href="https://arxiv.org/abs/2405.07857">Paper</a>
      
      </div>
      <div class="item">
        <img src='./media/hdr.gif' width="192" />
        <p>
        <b>HDR-Plenoxels: Self-Calibrating High Dynamic Range Radiance Fields</b><br/>
        <u>Kim Jun-Seong*</u>, Kim Yu-Ji*, Moon Ye-Bin, Tae-Hyun Oh<br/>
            <i>ECCV 2022</i><br/>
            <a href="https://hdr-plenoxels.github.io/">Project page</a> |
            <a href="https://arxiv.org/abs/2208.06787">Paper</a> |
            <a href="https://www.youtube.com/watch?v=LgZkVxgbOII&list=PLRg-WgV5P2lnv2JIo-RyhNPjlGD3aVmLj&index=5">Video</a>
            <span style="color:#BB2222;">- Presented in Learning 3D with Multi-View Supervision Workshop (3DMV) at CVPR 2023 </span><br/>
        </p>
      </div>

      <h2>Invited Talks</h2>
        <ul>
          <li> HDR-Plenoxels: High Dynamic Range Radiance Fields, NAVER, Korea, 2023</li>
        </ul>

      <h2>Academic Services</h2>
        <ul>
          <li> Conference Reviewer: ICCV (2023), CVPR (2024), ECCV (2024)<br/>
          <li> Workshop Reviewer : Neural Fields @ ICLR (2023)
        </ul>

      <div class="outro">
        template adapted from <a href="https://changilkim.com/"><font size="2">this website</font></a>
      </div>

    </div> <!-- container -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-5RW688RKHW"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-5RW688RKHW');
    </script>
  </body>
</html>
